{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to section 2 of this course \n",
    "Section two will focus more on the data part of python, whereas in the first sections we covered the foundation. When we work with data in Python, we can do so in many ways, such as using lists and dictoronairs filled with integers, strings and many more. However, one of the most common and easy ways to work with data in Python is by using **DataFrames**. Dataframes are very similair to 2-Dimensional Excel tables (or SQL output for that matter) and are thus very recognizable and interpretable for us humans. \n",
    "\n",
    "Unfortunately, by default, Python does not work with Dataframes out of the box. Fortunately, as we've seen two weeks ago, Python has many openly available additional libraries and modules. We've already worked very briefly with Numpy, probably the most commonly used library in Python. Second in this list is Pandas. Pandas is a python library designed to make many common types of data analysis straightforward and can be used to work with Dataframes, making it is incredibly useful for grouping data, aggregating and processing these groups, and reshaping data. Support for time-series analyses in Pandas is also strong with built-in date and time representations. Thus, long story short: Pandas is exactly what we need! Especially if we reconsider these monthly tasks that we used to do manually in Excel. Pandas to the rescue! Additionally, as Pandas is so commenly used, Google knows the answer to almost everything regarding pandas! \n",
    "\n",
    "Now, get yourself some cofee, because the real fun starts here!\n",
    "\n",
    "In this chapter we'll walk through the possibilities of Pandas using a car data set from an car action centre in Tulsa, Oklahoma, USA. P.S. Did you know that the state of Oklahoma sees the most tornados annually out of all US states? Also, it was the first state where a parking meter was installed. \n",
    "\n",
    "The data is in a .csv file, which is one of the most common ways to transfer data. As we've seen before, we can open .csv's with Excel and opt to split the cells based on the comma. However, hopefully after today, you'll give it a go using Python pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before we start doing anything, we have to make sure we import the correct libraries. Thus, we **import pandas** using exactly that statement. without important pandas, we'll get an error when we try to use it. If we use something from the library, we need to specifiy which library we're using. Thus, we start every specific pandas commando (code-line) with *pandas.*. We'll see in litle bit how this works out. Additionally, we need to know where our datafile is. In our case, we've put everything in the same folder, but we can go a level down in our folder structure if we wish to do so. \n",
    "\n",
    "Once we have pandas imported and know the location of our .csv datafile, we can start openen our data! We can do so using `'pandas.read_csv('document.csv')'` and assign it to a variable. If we assign it to variable, we still have to display our results, like we've seen with the print() statement. However, as displaying an entire dataframe with data can be quite cumbersome, there is an option to only show to the top or final section of the dataframe using `'.head()'` or `'.tail()'`. Let's see how this works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Reading and writing Pandas data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a data file into pandas\n",
    "data = pandas.read_csv(\"USA_cars_datasets.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, pandas perfectly shows us the top of our data, however, the first column seems to be double. That's because pandas automatically adds a index column. As we don't prefer to see that, we add `'index_col=0'` to our read_csv statement, making our dataframe a little bit more sleek. \n",
    "\n",
    "Also, lets use another trick. As typing *pandas* all day long might bore you, we can do a simpeler import statement.  `'import pandas'` and `'import pandas as pd'` both import pandas, but now we don't need to type `'pandas.read_csv(\"USA_cars_datasets.csv\")'` anymore, but only `'pd.read_csv(\"USA_cars_datasets.csv\")'` suffices, saving us potentially valuable time ! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"USA_cars_datasets.csv\", index_col=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also use pandas to open many (and I mean many different data sources). A relatively complete list can be found at this link: https://pandas.pydata.org/docs/user_guide/io.html. Remember, that many formats can be converted to another, further enlarging the possibilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Getting some simple information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have our dataframe called *data*. Bow we can extract some valuable information regarding our data. It is always important to get a better understanding of your data before working with it. For example, the dataframe may contain empty cells, seriously undermining your data analysis. Therefore, we do some basic analysis using the pandas methods `'.describe()'` and `'.info()'`. Let's see what these do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, these give some initial overviews on our dataframe. For example, we can see that the average (mean) price for a car at the Oklahoma action centre was $ 18.767, which makes sense. For example, if we saw that the average price was over an million, we might have to check our data source to verify whether or not that is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Selecting data in Pandas\n",
    "\n",
    "As with any data structure, it is important to be able to access subsets of the dataframe. Pandas has tools for accessing both rows and columns in a number of ways. Some methods show overlap, and often there are various ways to get to your selection. We'll stat bij selecting only specific columns, which is something we often need. For example, in the example below, I solely want the brand, model, year and price of a vehicle sold. \n",
    "\n",
    "Now, be aware. We're using double squared brackets here `' [] '`, for the simple reason that we pass a list with names into our pandas dataframe. Pandas coincidentally also uses squared brackets. Keep this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_smallest = data['brand']\n",
    "data_smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = data[[\"brand\", \"model\", \"year\", \"price\"]]\n",
    "data_small.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we only want the top 5, or a very specifiek range from our dataframe. We previously saw that we can select the top, using .head(), but we can also do so using **indexing**. With indexing, we select the indexnumber that we want. This is best explained using some examples.\n",
    "\n",
    "Using `'data[start_index:last_index] '`, we can select specific index numbers. If we don't take a start or last index, pandas assumes you mean either the begin or the end of the dataframe. Thus, using `'data[:5]'`, we get the exact same as using .head() that we saw previously. However, we can also enlarge this number, or add a start index. Please try for yourself in the frame below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index rows with just a set of indices (no comma)\n",
    "data[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine the trick of indexing, with the trick of column selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['model'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['model', 'brand', 'year']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose, we only want cars of one specific brand. We can easily do so by adding a conditional statement to our dataframe selection. We've already worked with conditional statements before. Hopefully you now see why we needed that basic knowledge. Let's select all cars of type ford, and then modify that same dataframe again to have only ford cars from 2000 and newer. \n",
    "\n",
    "Then I'll show you that we could also do this in one step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean indexing works. Notice how we only get ford's (and that this impacts our index)\n",
    "ford_data = data[data['brand'] == \"ford\"]\n",
    "ford_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ford_data_2019 = data[data['brand'] == \"ford\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do the exact same, but in only one statement. \n",
    "ford_data = data[(data['brand'] == \"ford\") & (data['year'] > 2000)]\n",
    "ford_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've worked with index numbers (left-hand side numbers) and column names. However, our rows can also have names. For example when we do a .describe() method and assign this to a variable name, we get a new dataframe with row names. Selecting rows works very similair, but now we need to pass the method `'.loc['rowname']'`. To add another weapon to our toolkit, we can use the `'.iloc[row index start: row index end]'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows can also have names:\n",
    "summary = ford_data.describe()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can index rows using the loc property:\n",
    "print(summary.loc['mean'])\n",
    "print()\n",
    "print(summary.loc[['mean', '50%']])\n",
    "print()\n",
    "print(summary.loc['25%':'75%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or by row index number using iloc:\n",
    "print(summary.iloc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Adding & analyzing data\n",
    "One of the benefits of pandas is the ease of data manipulations and the addition of new columns. Let's continue with our original car data. To ensure this data is clean, we do a read_csv again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"USA_cars_datasets.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to divide our car price paid during the auction with the mileage of the car and add that in a new column, we can easily do so with pandas. With the powerfull square brackets in pandas, we simply create a new columnname. `'data['new_column']'` should do the trick. If we simply run this as a command, we will get a new empty column with the title new_column. Easy piezy. However, now we actually want that column to contain something. We can do so by treating it as a variable and storing content in it (just like we do with a variable). Thus, we can store formulas, text and even do data modalations. Let's see if we can divide our car price with the mileage and put that into a new column. I can assure, we can, and relatively easy as well! \n",
    "\n",
    "Now, this is an powerfull tool, we can use this to do initial calculations in our dataframe without the need to extract data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_column'] = data['price'] / data['mileage']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pivot tables\n",
    "\n",
    "Pivot tables are a useful tool for summarizing data along different groupings. Pandas provides a wapper function called pivot_table for constructing these tables. We might know pivot tables already from excel, and you might reckon that they take some time getting used to. However, when used in the correct order, they are a very valuable method of analyzing and summarizing your (big) data. \n",
    "\n",
    "For example, in the pivot table below, we've specified the average price per brand for both salvage and clean vehicles. By default, pandas does the mean function. However, for example if we want the median, we simply add `'aggfunc=numpy.median'` to the line of code. There are however plenty of examples and there is no need to learn this by hard. If you ever need to, check a google search on 'pandas pivot median' and your problems are solved! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a pivot table:\n",
    "# columns parameter specifies the column types, index specifies the rows\n",
    "pd.pivot_table(data, values='price', index=[ 'title_status'], columns=['brand'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.1\n",
    "\n",
    "Can you make a pivot table that gives the average mileage for cars sold at the auction, but add the index of country so we can see whether or not Americans drive more than Canadians. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "\n",
    "One of the most powerful features in pandas is the ability to group dataframes by specific values and then perform calculations on those groups. These features are directly stolen from how R does this, a popular scientific programming language. However, after the groupby, we should tell python what to do with the text. The action is performs, is called an aggregation.  In the example below, we do a describe, but it can many different functions, depending on what you want. Again, there is no need to study them all and learn them by hard. If you know your way around the internet, you'll get there! :) For the sake of fun, i've added a list with as many aggregations that could think of. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('brand').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation\n",
    "A list of methods/functions that we can apply to our dataframe. By default to all columns, but we can also add it to one specific column.\n",
    "\n",
    "- count\n",
    "- cumcount\n",
    "- first\n",
    "- head\n",
    "- last\n",
    "- max\n",
    "- mean\n",
    "- median\n",
    "- cummax\n",
    "- cummin\n",
    "- cumprod\n",
    "- cumsum\n",
    "- describe\n",
    "- diff\n",
    "- ffill\n",
    "- fillna\n",
    "- idxmax\n",
    "- idxmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few examples:\n",
    "print(data.groupby('brand').size())\n",
    "print()\n",
    "print(data.groupby('price').max())\n",
    "print()\n",
    "print(data.groupby('mileage').agg([numpy.sum, numpy.mean, numpy.std]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2\n",
    "Using the car dataset, first select only the ford models from 2005 onwards. Then group by the type of car (saloon, MPV, etc) and then find the min, max, mean and std of these vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Joing and merging data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins using the Join function\n",
    "\n",
    "Join is a convenience function in pandas for joining two dataframes. It is a wrapper around the merge function. Suppose we have two dataframes, containing data on consumers with their unique membershipnumber (or klantnummer, hence they start with a K) as unique variable. I just create two dataframes manually. This might come in handy later. So store this as your reference. However, for this course, I presume you will retrieve most data either from excel or databases. I deem the changes that you will actually build a dataframe from scratisch using python, at most minimal. \n",
    "\n",
    "On purpose, the left side, or the first dataframe containing demographic data has K0 to K2. However, the right side, or the dataframe container specific membership details for retail customers, does not contain K1, but does include K3. This is also something we often see in real life examples. Not every costumer is in every database. \n",
    "\n",
    "Joining data in pandas (and in many other languages for that matter) requires an overlapping column, so that we know where to stick which information. This is called a **shared key**. By default, pandas uses the index, which in our case is the 'Klantnummer'. \n",
    "\n",
    "I highly recommend you take 10 minutes to understand this topic further by reading https://datacarpentry.org/python-ecology-lesson/05-merging-data/. There is alot of on information on this topic, and it is vital to understand that there are multiple ways to join datasets, and most variants have different outcomes. I hope that the folloiwing examples make it slighly clear. But, once again, practise makes perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just quickly making the dataframe from scratch.\n",
    "left = pandas.DataFrame({'Provincie': ['ZH', 'NH', 'LI'],\n",
    "                         'Telefoonnummer': ['088-2892888', '088-2892888', '088-2892888']},\n",
    "                        index=['K0', 'K1', 'K2'])\n",
    "right = pandas.DataFrame({'Totaal waarde bestellingen': ['754', '12', '6511'],\n",
    "                          'Aantal bestellingen': ['5', '1', '32']},\n",
    "                         index=['K0', 'K2', 'K3'])\n",
    "print(left)\n",
    "print()\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default is a left join: \n",
    "# all indexes from the left dataframe are kept as well as \n",
    "# any indexes in the right that occur in the left\n",
    "left.join(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also specify a right join\n",
    "left.join(right, how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or an outer join that keeps all indexes\n",
    "left.join(right, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or an inner join that keep all complete indexes\n",
    "left.join(right, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Joins using the merge function\n",
    "\n",
    "Another option to join two dataframes (or more for that matter) is to use merge. Merge has more options than Join. Specifically, it is possible to specify a key in both dataframes to merge based on, to specify multiple keys within each dataframe, etc. One really nice feature is to include a new column indicating which dataframe the row came from. However, most of the basic operations work perfectly with join, hence we won't dive to deep into merge. Speaking abouting diving: joins use merges underwater. So basically, we're using merges to join. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas are the answer to a lot! \n",
    "\n",
    "I would just like to state that pandas is the go-to tool for data analysis in python, and there is just no way to talk you through it in a couple of hours. It requires some practise to do all actions instantly and some google'ing to find all the inns-and-outs of pandas, but the functions are really intuitive and the sky is the limit. Personally, when I started working with Pandas I used the DataCamp pandas cheatsheet a lot, which can be found here: http://datacamp-community-prod.s3.amazonaws.com/dbed353d-2757-4617-8206-8767ab379ab3. This cheatsheet contains the basics (how to open a csv and excel file), but also comes in handy when you want to select data. Additionally, I think it is very valuable to browse throught the pandas website for some minutes, especially their 10 minute starter guide: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html. \n",
    "\n",
    "Pandas requires getting your hands dirty, but also allows you to start doing magnificient things! With the knowledge of pandas comes great power. Now you can start google'ing the fancy stuff! :) Obviously not before you've done some exercises. \n",
    "\n",
    "This chapter now continues with a small exercise, before showcasing some more fun stuf: time series and basic machine learning! We'll definitely no go into detail here (since my diploma won't be worth anything if we do). After this, the chapter concludes with some exercises. We haven't covered all the materials to succesfully finish these exercises, hence you will need to go on an adventure yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3\n",
    "\n",
    "Read in the three iris.txt datasets (a, b, and c) and combine them into a single dataframe. **TIP: Have a carefull look at the seperator in this document.** Feel free to use open the data sources first, to get an understanding of your data. Additionally, as stated before, Google has the answer to everything. So, if you're stuck, please do an efficient google search!\n",
    "\n",
    "Please consider for yourself: Did you use concatinate, merge, join, or another function? Why? What aspects of the data would make you change your choice? And would other methods also work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Applying functions to our dataframe!\n",
    "One of the most powerfull actions in pandas is that you can actually call a function on a dataframe. We do this, by adding `.apply(function_name)` to our dataframe (which can also be a specific column). To show you this, we quickly import the car sales data from the US again, and then make a very simple function stating that cars sold < 1000 are cheap, 1000 - 10000 are priced normal and > 10000 are expensive. From a data analysis perspective, this obviously makes no sense, but hope it helps in understanding the .apply method in pandas. A very usefull tool. \n",
    "\n",
    "Hopefully by now, you also understand why we needed the basics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data. \n",
    "data = pd.read_csv(\"USA_cars_datasets.csv\", index_col=0)\n",
    "\n",
    "# make the simple function\n",
    "def pricing_name(x):\n",
    "    if x < 1000:\n",
    "        return \"Cheap\"\n",
    "    elif x >= 1000 and x < 10000:\n",
    "        return \"Normal value\"\n",
    "    else: \n",
    "        return \"Expensive!\"\n",
    "    \n",
    "# apply the function to the data \n",
    "data['new_column'] = data['price'].apply(pricing_name)  \n",
    "\n",
    "\n",
    "# The new column is now at the very end. \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in dataframes are very powerfull and can form the backbone of your analysis. Remember that functions are designed to make replicable tasks go away. Please use this to your advantage! You only need to make a function once (in your notebook, but you can always store it locally) and you can use it many times! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Some more fun!  *\n",
    "As stated before, pandas is a very versitile library. It includes waaaaaay more then we have discussed above and hope you can use pandas to actually solve one of your personal repetitive tasks! Below here, I'll show you the apple stock data up until 2014. In this example, we import data from an url directly. So, hence you see the versitality again of pandas. We'll also quickly import a library that we haven't discussed before, called matplotlib. It's a mathamatical plotting library, and is one of the most commonly used libraries to plot.More on this library, next week!\n",
    "\n",
    "If you want to, I recommend that you pass through it, and enjoy the power of pandas. Within minutes, we have the shares of Apple mapped in a reproducable way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/09_Time_Series/Apple_Stock/appl_1980_2014.csv'\n",
    "apple = pd.read_csv(url)\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data column has been importated as string, and we actually want it to be in the 'data' format. \n",
    "# Luckily, this is also something pandas can do for us!\n",
    "\n",
    "apple.Date = pd.to_datetime(apple.Date)\n",
    "\n",
    "# Lets allso set the index to the date column, which makes it slighly leaner\n",
    "# and then we sort it in an ascending way!\n",
    "apple = apple.set_index('Date')\n",
    "apple.sort_index(ascending = True)\n",
    "\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# makes the plot and assign it to a variable\n",
    "appl_open = apple['Adj Close'].plot(title = \"Apple Stock\")\n",
    "\n",
    "# changes the size of the graph\n",
    "fig = appl_open.get_figure()\n",
    "fig.set_size_inches(13.5, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Excersises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.0\n",
    "Can you just select the following four (price, brand, model, year) columns in the car dataset (USA_cars_datasets.csv)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.1\n",
    "Open the full iris.txt document using pandas, and return a dataframe that gives the mean value per species for every value in the data. Be aware of the the seperators in the data.\n",
    "\n",
    "HINT: This can be done using a group by. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.2\n",
    "Open the dataset from the following url: `https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user` in pandas using `|` as a seperator and with as index column `'user_id'`. This will give us a dataset bij Mark Hamminga, who collected data on American occupation per zip code. However, this is just a long list and doesn't tell us anything. Can you extract the following in pandas in this dataset? \n",
    "- The average age of somebody being reitred?\n",
    "- The average age for both males and females in any occupation? *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_table('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n",
    "                      sep='|', index_col='user_id')\n",
    "# quick way, can also first make a Dataframe selecting retireds only.\n",
    "# users.groupby('occupation').age.mean()\n",
    "# or\n",
    "users_retired = users[users['occupation'] == 'retired']\n",
    "users_retired['age'].mean()\n",
    "\n",
    "# or \n",
    "# users_retired = users[users['occupation'] == 'retired']\n",
    "# users_retired.age.mean()\n",
    "\n",
    "\n",
    "\n",
    "# * users.groupby(['occupation', 'gender']).age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.3\n",
    "Somehow, we've found some customer data of our organization (no worries though... just made this up). However, as is often the case, the dataset is lacking the card color of the member. Unforunately, you need this to send them a personalized email using their klant_id. Can you make a dataframe containing the at least the membership number and card color for every member? There are two .csv files that can help you do so: jarenlid.csv and lidmaatschap.xlsx. How to open an .xlsx is all up to you!\n",
    "\n",
    "Thus, we need multiple things. First consider your plan of approach! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.4\n",
    "Using the Datasets from excersise 3, can you give the following information? \n",
    "\n",
    "- The average age of both males and females?\n",
    "- The total number of logins\n",
    "- The average membership length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.5\n",
    "Just consider for yourself: is there an excel task that you can do in Python? Now is the time to give it a go! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersise 10.8.6 *\n",
    " - With the Saledata.xlsx, create a pandas  Pivot table and find the total sale amount region wise, manager wise.\n",
    " - Additionally, create a Pivot table and find the total sale amount region wise, manager wise, sales man wise.\n",
    " - Also, create a Pivot table and find manager wise, salesman wise total sale and also display the sum of all sale amount at the bottom.\n",
    " \n",
    "Pandas is one of the most valuable tools in Python for data analysis and in no way could I inform you in just one lecture about the possibilities. As some of you stated that it was hard to finish all the excersises within 3 hours, I will not add additional ones here. If you want to practise more, I highly recommend the following: https://www.w3resource.com/python-exercises/pandas/practice-set1/index.php Some of the excersises are even from W3resources and are very usefull. Just pic a nice one and give it a try! \n",
    "\n",
    "Additionally, I highly recommend you start working on your own little pet project. It can be something related to your current hobby or work. There are a lot of datasets out there publicly available, waiting for you to use! For example, I was interested in wiskey as an alternative investment (so, actually buying wiskey, in order to later sell them for a premium). Somehow, there are plenty of wiskey datasets out there! Guess the internet likes booze! \n",
    "\n",
    "It can be really helpfull to actually get your hands dirty and highly recommend doing so! Especially if you have some time left, but off course, only if you think it is a valuable investment of your time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
